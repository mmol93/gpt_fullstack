{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, max_token_limit=120, return_messages=True, memory_key=\"history\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "# RunnablePassthrough을 사용하여 필요한 데이터를 먼저 load하고 그 값을 history라는 이름으로 prompt에 전달한다.\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "\n",
    "invoke_chain(\"My name is nico\")\n",
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "# 받은 문자열을 ,을 기준으로 리스트로 만든다.\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        # list 각 항목에 다시 str.strip 함수를 적용\n",
    "        # str.strip 함수는 각 문자열의 앞 뒤 공백을 제거한다.\n",
    "        return list(map(str.strip, items))\n",
    "\n",
    "\n",
    "parser = CommaOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a list generating machine. Everythig you are asked will be answered with a comma seperated list of max {max_items}. Do not reply with anything else.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain은 PromptTemplate | Model | OutputParser 로 만들 수 있다.\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "chain.invoke({\"max_items\": 5, \"question\": \"what are animals?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.openai import OpenAI  # 기본값으로 davinchi-003를사용\n",
    "from langchain.chat_models import ChatOpenAI  # 기본값으로 GTP3.5-Turbo를 사용\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage, BaseOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "# streaming=True, callbacks=[StreamingStdOutCallbackHandler()] 옵션을 넣어주면 실시간 출력을 볼 수 있다.\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "# 답번의 예시\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about Japan?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Tokyo\n",
    "        Languae: Japanese\n",
    "        Food: Sushi\n",
    "        Currency: Yen\n",
    "        \"\"\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# 답변 양식에 대한 prompt\n",
    "example_prompt = PromptTemplate.from_template(\"Human:{question}|nAI:{answer}\")\n",
    "\n",
    "# 질문에 대한 prompt 작성\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    # suffix가 유저의 질문 부분임\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Korea\")\n",
    "# 결과: 'Human:What do you know about Japan?|nAI:\\n  I know this:\\n  Capital: Tokyo\\n  Languae: Japanese\\n  Food: Sushi\\n   :Currency: Yen\\n   \\n\\nHuman: What do you know about Korea?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "# 랭체인을 사용해 얻은 값을 자동으로 메모리에 캐싱하도록 한다.\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a list generating machine. Everythig you are asked will be answered with a comma seperated list of max {max_items}. Do not reply with anything else.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 다음 chain은 PromptTemplate | Model | OutputParser 형태로 이루어져 있다.\n",
    "chain = template | chat | CommaOutputParser()\n",
    "# 결과: ['Mammals', 'Birds', 'Reptiles', 'Amphibians', 'Fish']\n",
    "chain.invoke({\"max_items\": 5, \"question\": \"what are animals?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# get_openai_callback()를 사용하면 openAI를 사용한 결과를 받아올 수 있다.(챗 결과랑 다른거임)\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju\")\n",
    "    b = chat.predict(\"What is the recipe for bread\")\n",
    "    print(a, \"\\n\")\n",
    "    print(b, \"\\n\")\n",
    "    # 위 처리에 소요된 비용을 출력\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.openai import OpenAI  # 기본값으로 davinchi-003를사용\n",
    "from langchain.chat_models import ChatOpenAI  # 기본값으로 GTP3.5-Turbo를 사용\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage, BaseOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "\n",
    "# streaming=True, callbacks=[StreamingStdOutCallbackHandler()] 옵션을 넣어주면 실시간 출력을 볼 수 있다.\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"Japan\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Tokyo\n",
    "        Languae: Japanese\n",
    "        Food: Sushi\n",
    "        :Currency: Yen\n",
    "        \"\"\",\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", \"What do you know about {country}?\"), (\"ai\", \"{answer}\")]\n",
    ")\n",
    "\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # ChatPromptTemplate을 사용해서 답변 양식을 만든다.\n",
    "    example_prompt=example_prompt,\n",
    "    # 실제 답변의 예시를 만든다.\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert\"),\n",
    "        # 이전에 니가 이런식으로 답변했다는 것을 example_prompt로 예를 들어준다.\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Lee, I live in South Korea\", \"Wow, that is so cool!\")\n",
    "add_message(\"South Korea is getting fallen.\", \"what? why?\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm LEE, I live in South Korea\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"input\": \"who is LEE\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"LEE likes kimchi\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"inputs\": \"what does LEE like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Winston lives in Victory Mansions, specifically on the seventh floor of the building.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = TextLoader(\"./files/textbook.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "# 해당 함수에서 돌려주는 값은 context에 들어가는 값이기 때문에 하나의 string을 반환해야한다.\n",
    "# 그렇기 때문에 inputs에 들어 있는 모든 데이터를 하나의\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\n",
    "                \"context\": doc.page_content,\n",
    "                \"question\": question,\n",
    "            }\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "# documents: vectorstore에서 받은 Documents\n",
    "# question: vectorstore에서 값을 받기 위한 질문\n",
    "# RunnableLambda를 사용하면 chain의 어느 위치에서도 함수 실행 가능\n",
    "# RunnableLambda에서 map_docs 함수를 실행하는데 자동으로 map_docs에 필요한 파라미터로써 앞에서 실시한 \"documents\": retriever, \"question\": RunnablePassthrough() 값이 들어간다.\n",
    "# 참고로 \"documents\": retriever, \"question\": RunnablePassthrough()은 사전형 데이터로 inputs 파라미터에 들어간다.\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 요약받은 Doument(=context)에 prompt를 적용시켜 llm을 한다.\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"Where does Winston live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SYSTEM:  ### System:\n",
      "You are an AI assistant that follows instruction extremely well. Help as much as you can.\n",
      "USER: hello\n",
      "ASSISTANT:  Hello! How may I assist you today?\n",
      "-----------\n",
      "\n",
      "USER: write me a short poem\n",
      "ASSISTANT:  Sure, here's a short poem for you: \n",
      "\n",
      "Beneath the bright sun, \n",
      "The flowers bloom and dance. \n",
      "The birds sing sweetly, \n",
      "As butterflies flutter by. \n",
      "\n",
      "In this world of chaos, \n",
      "We find peace and calm. \n",
      "And in each moment we seize, \n",
      "Our lives are filled with light.\n",
      "-----------\n",
      "\n",
      "USER: thank you\n",
      "ASSISTANT:  You're welcome! Is there anything else I can help you with?\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(model_name='orca-mini-3b-gguf2-q4_0.gguf')\n",
    "with model.chat_session():\n",
    "    response1 = model.generate(prompt='hello', temp=0)\n",
    "    response2 = model.generate(prompt='write me a short poem', temp=0)\n",
    "    response3 = model.generate(prompt='thank you', temp=0)\n",
    "    print()\n",
    "\n",
    "    for message in model.current_chat_session:\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "        \n",
    "        if role == 'system':\n",
    "            print(f\"SYSTEM:  {content}\")\n",
    "        elif role == 'user':\n",
    "            print(f\"USER: {content}\")\n",
    "        else:\n",
    "            print(f\"ASSISTANT: {content}\")\n",
    "            print(\"-----------\")\n",
    "            print(\"\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
