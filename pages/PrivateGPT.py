import streamlit as st
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import TextLoader, PyPDFLoader
from langchain.embeddings import OllamaEmbeddings, CacheBackedEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOllama
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.callbacks.base import BaseCallbackHandler


class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    # *argsëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë°›ëŠ” ëª¨ë“  ì¸ìë¥¼ ì˜ë¯¸
    # **kwargsëŠ” key-value í˜•íƒœë¡œ ë°›ëŠ” ëª¨ë“  ì¸ìë¥¼ ì˜ë¯¸(ì˜ˆ: a=1, b=2...)
    def on_llm_start(self, *args, **kwargs):
        # aiê°€ ë§í•˜ëŠ” ê²ƒì„ ë‹´ê¸° ìœ„í•œ ë¹ˆ ìƒì ìœ„ì ¯ì„ ë§Œë“ ë‹¤.
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        # aië¡œ ë‹µë³€ ì‘ì„±ì´ ëë‚¬ìœ¼ë©´ ë‹µë³€ì„ ì €ì¥í•œë‹¤.
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *arg, **kwargs):
        # í† í°ì„ ë°›ì„ ë•Œë§ˆë‹¤ ê¸°ì¡´ ë©”ì‹œì§€ì— ë¶™ì´ë„ë¡ í•œë‹¤.
        self.message += token
        self.message_box.markdown(self.message)


# í•¨ìˆ˜ì—ì„œ ë°›ì€ fileì´ ë™ì¼í•œ fileì´ë¼ë©´ í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šê³  ê¸°ì¡´ ë°˜í™˜ê°’ì„ ë‹¤ì‹œ ë°˜í™˜í•œë‹¤.
@st.cache_data(show_spinner="Embedding files....")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/private_files/{file.name}"

    # í•´ë‹¹ ìœ„ì¹˜ì— íŒŒì¼ì„ ì €ì¥í•œë‹¤.
    with open(file_path, "wb") as f:
        # (ë””ë²„ê¹…ìš©)
        f.write(file_content)

    # í•´ë‹¹ ìœ„ì¹˜ì— ìºì‹±í•œ ë°ì´í„°ë¥¼ ì €ì¥
    cache_dir = LocalFileStore(f"./.cache/private_embeddings/{file.name}")

    # chunk_sizeëŠ” í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•  ë•Œ ê° ì¡°ê°ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •í•œë‹¤.
    splitter = CharacterTextSplitter(separator="\n", chunk_size=500, chunk_overlap=100)
    if ".txt" in file.name: 
        text_loader = TextLoader(f"./.cache/files/{file.name}")
    elif ".pdf" in file.name:
        text_loader = PyPDFLoader(f"./.cache/files/{file.name}")
    docs_from_text_loader = text_loader.load_and_split(text_splitter=splitter)
    embeddings = OllamaEmbeddings(
        model="mistral:latest"
    )

    # ì„ë² ë””ë“œí•œ ë°ì´í„°ë¥¼ ì¼€ì‹±ì²˜ë¦¬
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    # Chromaë¼ëŠ” Vector Storeë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥ë˜ìˆëŠ” ì„ë² ë””ë“œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤.
    vectorstore = FAISS.from_documents(docs_from_text_loader, cached_embeddings)

    return vectorstore.as_retriever()


def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})


def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message=message, role=role)


def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message=message["message"],
            role=message["role"],
            save=False,
        )


def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)


llm = ChatOllama(
    model="mistral:latest",
    temperature=0.2,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],
)

st.set_page_config(
    page_title="PrivateGPT",
    page_icon="ğŸ“ƒ",
)

st.title("PrivateGPT")

st.markdown(
    """
Welcome!

Use this chat bot with your files on the sidebar!

---

"""
)

prompt = ChatPromptTemplate.from_template(
    """[INST]Answer the question using ONLY the following context and not your training data. If you don't know the answer just say you don't know. DON'T make anything up[/INST].
    
    Context: {context}
    Question:{question}
    """
)

with st.sidebar:
    # file_uploader ìœ„ì ¯ì„ ì‚¬ìš©í•´ì„œ íŒŒì¼ ì—…ë¡œë“œë¥¼ ë°›ì„ ìˆ˜ ìˆê²Œ í•œë‹¤.
    file = st.file_uploader("Upload a .txt or .pdf file.", type=["pdf", "txt"])

if file:
    retriever = embed_file(file=file)
    paint_history()
    message = st.chat_input("Ask anything!")
    if message:
        send_message(message=message, role="human")
        # chainì— í•„ìš”í•œê±´ promptì— ë„£ì–´ì¤„ ê°’ | prompt | llm ì„ì„ ìƒê°í•˜ê³  ë§Œë“ ë‹¤.
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            # chain.invokeë¥¼ chat_message ì•ˆì— ë„£ì–´ì„œ responseê°€ chat_messageë¡œ ë‚˜ì˜¤ê²Œ í•œë‹¤.
            # ê·¸ë ‡ê²Œ í•˜ë©´ ChatCallbackHandler ì•ˆì— ìˆëŠ” ê²ƒë“¤ì´ ë˜‘ê°™ì´ chat_message ì•ˆì—ì„œ ì‹¤í–‰ë˜ëŠ”ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤.
            chain.invoke(message)

else:
    st.session_state["messages"] = []
