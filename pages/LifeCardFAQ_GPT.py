from langchain.document_loaders import SitemapLoader
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
import streamlit as st
import os
from langchain.storage import LocalFileStore

project_root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
faq_caching_file_path = os.path.join(project_root_path, '.cache/lifecard')
faq_caching_file_path_langchain = LocalFileStore("./.cache/lifecard/")
life_faq_sitemap_url = "https://lifecard.dga.jp/sitemap.xml"

llm = ChatOpenAI(
    temperature=0.1,
)

# Map Re-rankë¥¼ ì‹¤ì‹œí•˜ëŠ” prompt
answers_prompt = ChatPromptTemplate.from_template(
    """
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ä½¿ç”¨ã—ã¦è¡Œã£ã¦ãã ã•ã„ã€‚åˆ†ã‹ã‚‰ãªã„å ´åˆã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹ã ã‘ã§ã€é©å½“ãªå›ç­”ã¯ã—ãªã„ã§ãã ã•ã„ã€‚æ¬¡ã«ã€å›ç­”ã«0ã‹ã‚‰5ã®ç¯„å›²ã§ç‚¹æ•°ã‚’ã¤ã‘ã¦ãã ã•ã„ã€‚å›ç­”ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ã„ã‚‹å ´åˆã¯é«˜ã„ç‚¹æ•°ã‚’ã€ãã†ã§ãªã„å ´åˆã¯ä½ã„ç‚¹æ•°ã‚’ã¤ã‘ã¦ãã ã•ã„ã€‚å›ç­”ã®ç‚¹æ•°ã¯å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚ãŸã¨ãˆ0ç‚¹ã§ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {context}

ä¾‹: è³ªå•: æœˆã¯ã©ã®ãã‚‰ã„ã®è·é›¢ã«ã‚ã‚Šã¾ã™ã‹?

å›ç­”: æœˆã¯384,400kmã®è·é›¢ã«ã‚ã‚Šã¾ã™ã€‚
ç‚¹æ•°: 5
è³ªå•: å¤ªé™½ã¯ã©ã®ãã‚‰ã„ã®è·é›¢ã«ã‚ã‚Šã¾ã™ã‹?
å›ç­”: ã‚ã‹ã‚Šã¾ã›ã‚“
ç‚¹æ•°: 0
ã‚ãªãŸã®ç•ªã§ã™! 

è³ªå•: {question}
"""
)


def parse_page(soup):
    target_soup = soup.find('div', class_="faq-box")
    print(target_soup.text)
    return (
        str(target_soup.get_text())
        .replace("\n", " ")
        .replace("\xa0", " ")
        .replace("CloseSearch Submit Blog", "")
    )


@st.cache_data(show_spinner="Webãƒšãƒ¼ã‚¸ã‹ã‚‰FAQãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ä¸­")
def load_website(url):
    st.write(faq_caching_file_path)
    cache_file_list = os.listdir(faq_caching_file_path)
    # ìºì‹±ëœ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì›¹ì—ì„œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë¡œë“œí•œë‹¤.
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=200)
    loader = SitemapLoader(url, parsing_function=parse_page)
    loader.requests_per_second = 2
    docs = loader.load_and_split(text_splitter=splitter)
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(OpenAIEmbeddings(), faq_caching_file_path_langchain)
    vector_store = FAISS.from_documents(docs, cached_embeddings)
    return vector_store.as_retriever(search_kwargs={"k": 3})
    # if cache_file_list:
    #     # ìºì‹±ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
    #     cached_embeddings = CacheBackedEmbeddings.from_bytes_store(OpenAIEmbeddings(), faq_caching_file_path_langchain)
    #     cached_docs = cached_embeddings.docs
    #     vector_store = FAISS.from_documents(cached_docs, cached_embeddings)
    #     return vector_store.as_retriever(3)
    # else:
        

st.set_page_config(
    page_title="LifeCardFAQ GPT",
    page_icon="ğŸ’³",
)


st.markdown(
    """
    # LifeCardFAQ GPT
            
    ã‚ˆã†ã“ãã€Œã‚ˆãã‚ã‚‹è³ªå•ã€ã®GPTã¸
"""
)
question = st.chat_input(
    "è³ªå•å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
)

if question:
    retriever = load_website(life_faq_sitemap_url)
    chain = ({
        "context": retriever,
        "question": RunnablePassthrough()
    }) | answers_prompt | llm

    result = chain.invoke(question)
    st.markdown(result.content)
